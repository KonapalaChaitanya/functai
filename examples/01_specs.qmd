---
title: "FunctAI Specification: The Function-is-the-Prompt Paradigm"
format:
  html:
    code-fold: true
---

# FunctAI Specification: The Function-is-the-Prompt Paradigm

## Overview

The core philosophy of FunctAI is: **The function definition *is* the prompt, and the function body *is* the program definition.**

FunctAI transforms Python functions into AI-powered operations through a simple decorator pattern. Write functions with natural language docstrings and type hints, and let AI handle the implementation.

## 1\. Core Concepts

### The @ai Decorator

Transform any function into an AI-powered operation by adding the @ai decorator. The function's docstring becomes part of the instructions in the prompt, parameters become inputs, and the return type defines the expected output.

```{python}
from functai import ai, _ai, configure

configure(lm = "gpt-4.1")

@ai
def summarize(text):
    """Summarize the given text in one sentence."""
    return _ai  # AI generated the summary

summarize("Long article about climate change...")
# Expected: "Climate change poses significant risks to global ecosystems and requires immediate action."
```

### The _ai Sentinel

The _ai object is a special sentinel used within an @ai function to represent values generated by the AI. It acts as a proxy, deferring the actual LM execution until the value is needed.

Inside an @ai function, `_ai` represents the AI-generated value. You can return it directly or post-process it:

###### Directly
```{python}
@ai
def extract_price(description: str) -> float:
    """Extract the price from a product description."""
    return _ai  # Direct return

extract_price("The item was 0.99")
```

###### Post-processing

```{python}
@ai
def sentiment_score(text: str) -> float:
    """Returns a sentiment score between 0.0 (negative) and 1.0 (positive)."""
    # _ai behaves like a float here due to the return type hint
    score = _ai
    # Post-processing: ensure the score is within bounds
    return max(0.0, min(1.0, float(score)))

sentiment_score("I think that FunctAI is amazing!")
```

<all specs drafts>

# FunctAI: Natural Language Functions for Python

## Overview

FunctAI transforms Python functions into AI-powered operations through a simple decorator pattern. Write functions with natural language docstrings and type hints, and let AI handle the implementation.

## Core Concepts

### The `@ai` Decorator

Transform any function into an AI-powered operation by adding the `@ai` decorator. The function's docstring becomes the prompt, parameters become inputs, and the return type defines the expected output.

```python
from functai import ai

@ai
def sentiment(text: str) -> str:
    """Analyze the sentiment of the given text.
    Return 'positive', 'negative', or 'neutral'."""
    ...

# Usage
result = sentiment("I love this new feature!")
# Output: "positive"
```

### The `_ai` Sentinel

Inside an `@ai` function, `_ai` represents the AI-generated value. You can return it directly or post-process it:

```python
@ai
def extract_price(description: str) -> float:
    """Extract the price from a product description."""
    return _ai  # Direct return

@ai  
def get_keywords(article: str) -> list[str]:
    """Extract 5 key terms from the article."""
    keywords = _ai
    return [k.lower() for k in keywords]  # Post-processing
```

## Type System & Outputs

### Basic Types

FunctAI respects Python type hints and ensures outputs match the expected types:

```python
@ai
def calculate(expression: str) -> int:
    """Evaluate the mathematical expression."""
    return _ai

result = calculate("What is 15 times 23?")
# Output: 345 (as int)

@ai
def parse_data(text: str) -> dict:
    """Extract structured data from the text."""
    return _ai

data = parse_data("John Doe, age 30, engineer")
# Output: {"name": "John Doe", "age": 30, "profession": "engineer"}
```

### Multiple Outputs

Define multiple outputs using type annotations and the `_ai` sentinel:

```python
@ai
def analyze_customer_review(review: str) -> str:
    """Analyze a customer review comprehensively."""
    
    # Declare additional outputs with descriptions
    sentiment: str = _ai["What is the overall sentiment?"]
    score: int = _ai["Rating from 1-10"]
    key_points: list[str] = _ai["Main points mentioned"]
    
    # Return primary output
    return f"Sentiment: {sentiment}, Score: {score}/10"

result = analyze_customer_review("This product exceeded my expectations...")
# Primary output: "Sentiment: positive, Score: 9/10"
```

### Structured Outputs with Dataclasses

```python
from dataclasses import dataclass
from typing import List

@dataclass
class ProductInfo:
    name: str
    price: float
    features: List[str]
    in_stock: bool

@ai
def extract_product(description: str) -> ProductInfo:
    """Extract product information from the description."""
    return _ai

info = extract_product("iPhone 15 Pro - $999, 5G, titanium design, available now")
# Output: ProductInfo(name="iPhone 15 Pro", price=999.0, 
#                     features=["5G", "titanium design"], in_stock=True)
```

## Configuration

### Global Settings

```python
from functai import configure

# Set project-wide defaults
configure(
    lm="gpt-4",  # or "claude-3", "llama-3", etc.
    temperature=0.7,
    adapter="json",  # Output format: "json" or "chat"
)
```

### Per-Function Configuration

```python
@ai(temperature=0.2, lm="gpt-4-turbo")
def legal_analysis(document: str) -> str:
    """Provide legal analysis of the document."""
    return _ai

@ai(temperature=0.9, lm="claude-3-opus")  
def creative_story(prompt: str) -> str:
    """Write a creative story based on the prompt."""
    return _ai
```

### Context Managers

```python
from functai import defaults

# Temporarily override settings
with defaults(temperature=0.1, lm="gpt-4"):
    result = my_ai_function("input")
```

## Advanced Features

### Stateful Conversations

Enable conversation memory to maintain context across calls:

```python
@ai(stateful=True)
def assistant(message: str) -> str:
    """Helpful AI assistant that remembers context."""
    return _ai

assistant("My name is Alice")
# Output: "Nice to meet you, Alice!"

assistant("What's my name?")
# Output: "Your name is Alice."

# Clear history when needed
assistant.state.clear()
```

### Chain of Thought Reasoning

```python
@ai(module="cot")  # Enable chain-of-thought
def solve_problem(problem: str) -> str:
    """Solve the problem step by step."""
    
    reasoning: str = _ai["Show your reasoning step-by-step"]
    answer: str = _ai["Final answer"]
    
    return answer

result = solve_problem("If a train travels 60 mph for 2.5 hours...")
# The AI will think through the problem before answering
```

### Tool Use & Function Calling

```python
def search_web(query: str) -> str:
    """Search the web for information."""
    # Implementation
    return results

def calculate(expression: str) -> float:
    """Perform mathematical calculations."""
    # Implementation
    return result

@ai(tools=[search_web, calculate], module="react")
def research_assistant(question: str) -> str:
    """Answer questions using available tools."""
    return _ai

answer = research_assistant("What's the GDP of France divided by its population?")
# The AI will use tools to search for data and perform calculations
```

### Optimization & Few-Shot Learning

```python
# Create training examples
trainset = [
    {"text": "I love it!", "expected": "positive"},
    {"text": "Terrible service", "expected": "negative"},
    {"text": "It's okay", "expected": "neutral"},
]

# Optimize the function with examples
sentiment_analyzer.opt(trainset=trainset)

# The function now includes learned examples for better performance
result = sentiment_analyzer("Not bad at all")
# Output: "positive" (informed by examples)

# Undo optimization if needed
sentiment_analyzer.undo_opt()
```

## Inspection & Debugging

### Preview Prompts

```python
from functai import format_prompt

# See what prompt will be sent
prompt_info = format_prompt(my_function, input1="value1", input2="value2")
print(prompt_info["render"])
```

### View Signature

```python
from functai import signature_text

print(signature_text(my_function))
# Shows inputs, outputs, types, and descriptions
```

### Inspect History

```python
from functai import inspect_history_text

# View conversation history for debugging
print(inspect_history_text())
```

## Best Practices

### 1. Clear Docstrings
Write descriptive docstrings that clearly explain what the function should do:

```python
@ai
def summarize(text: str, max_words: int = 100) -> str:
    """Summarize the text in a concise manner.
    
    Focus on key points and main ideas.
    Keep the summary under the specified word limit.
    Use clear, simple language.
    """
    return _ai
```

### 2. Type Hints Are Contracts
Always provide type hints - they guide the AI and validate outputs:

```python
from typing import List, Dict, Optional

@ai
def parse_csv_row(row: str) -> Dict[str, Optional[str]]:
    """Parse CSV row into a dictionary."""
    return _ai
```

### 3. Post-Processing for Reliability
Add validation and post-processing for critical applications:

```python
@ai
def extract_email(text: str) -> Optional[str]:
    """Extract email address from text."""
    email = _ai
    
    # Validate email format
    import re
    if email and re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', email):
        return email.lower()
    return None
```

### 4. Use Lower Temperatures for Consistency
For deterministic tasks, use low temperature:

```python
@ai(temperature=0.1)
def extract_facts(document: str) -> List[str]:
    """Extract factual statements from the document."""
    return _ai
```

### 5. Leverage State Wisely
Use stateful mode for conversational interfaces, but clear state when starting new contexts:

```python
chatbot = ai(stateful=True)(lambda msg: "Respond helpfully")

# New conversation
chatbot.state.clear()
response = chatbot("Hello!")
```

## Complete Examples

### Customer Service Bot

```python
@ai(stateful=True, temperature=0.7)
def customer_service(message: str) -> str:
    """Professional customer service representative.
    
    Be helpful, empathetic, and solution-oriented.
    Remember previous messages in the conversation.
    Offer specific solutions when possible.
    """
    
    sentiment: str = _ai["Customer sentiment: positive/negative/neutral"]
    needs_escalation: bool = _ai["Should this be escalated to a human?"]
    
    response = _ai
    
    if needs_escalation:
        response += "\n\n[Flagged for human review]"
    
    return response

# Usage
response = customer_service("I've been waiting 2 weeks for my order!")
print(response)
```

### Data Extraction Pipeline

```python
from dataclasses import dataclass
from typing import List, Optional
from datetime import date

@dataclass
class Invoice:
    invoice_number: str
    date: date
    total: float
    items: List[str]
    tax_rate: Optional[float]

@ai(temperature=0.1)
def extract_invoice(document: str) -> Invoice:
    """Extract invoice information from document text.
    
    Parse all relevant fields accurately.
    Convert amounts to float.
    Extract line items as a list.
    """
    return _ai

# Chain multiple AI functions
@ai
def validate_invoice(invoice: Invoice) -> bool:
    """Validate if the invoice data is complete and reasonable."""
    return _ai

@ai
def summarize_invoice(invoice: Invoice) -> str:
    """Create a brief summary of the invoice."""
    return _ai

# Pipeline
document = load_document("invoice.pdf")
invoice = extract_invoice(document)
if validate_invoice(invoice):
    summary = summarize_invoice(invoice)
    print(summary)
```

### Research Assistant

```python
@ai(tools=[search_web, read_paper, calculate], module="react")
def research_assistant(query: str) -> str:
    """Advanced research assistant.
    
    Use available tools to gather information.
    Synthesize findings from multiple sources.
    Provide citations when possible.
    Be thorough but concise.
    """
    
    research_notes: List[str] = _ai["Key findings from research"]
    confidence: str = _ai["Confidence level: high/medium/low"]
    sources: List[str] = _ai["Sources consulted"]
    
    answer = _ai
    
    # Add metadata
    answer += f"\n\nConfidence: {confidence}"
    answer += f"\nSources: {', '.join(sources)}"
    
    return answer

result = research_assistant(
    "What are the latest breakthroughs in quantum computing?"
)
```

## Error Handling

FunctAI provides clear error messages and graceful fallbacks:

```python
@ai
def risky_extraction(text: str) -> dict:
    """Extract structured data."""
    try:
        data = _ai
        # Validate structure
        assert "required_field" in data
        return data
    except Exception as e:
        # Fallback behavior
        return {"error": str(e), "raw_text": text}
```

## Performance Considerations

- **Batching**: Process multiple items efficiently
- **Caching**: Results are cached within the same session
- **Async Support**: Use with async/await for concurrent operations
- **Token Optimization**: Minimize prompt size while maintaining clarity

```python
# Efficient batch processing
@ai
async def process_batch(items: List[str]) -> List[dict]:
    """Process multiple items efficiently."""
    return _ai

# With caching
from functools import lru_cache

@lru_cache(maxsize=100)
@ai(temperature=0.1)
def cached_extraction(text: str) -> dict:
    """Extract data with caching."""
    return _ai
```

## Migration Guide

### From OpenAI Functions
```python
# Before
response = openai.chat.completions.create(
    model="gpt-4",
    messages=[{"role": "user", "content": prompt}],
    functions=[...],
)

# After  
@ai(lm="gpt-4")
def my_function(input: str) -> str:
    """Function description."""
    return _ai
```

### From LangChain
```python
# Before
chain = LLMChain(llm=llm, prompt=prompt_template)
result = chain.run(input=input_text)

# After
@ai
def my_chain(input: str) -> str:
    """Template instructions here."""
    return _ai
```

## Conclusion

FunctAI bridges the gap between natural language and programming, making AI integration as simple as writing a function. By combining Python's expressiveness with AI's capabilities, developers can build intelligent applications with minimal boilerplate while maintaining full control over the process.







# FunctAI Library Specification

## Overview

FunctAI is a Python library that transforms regular Python functions into AI-powered executables using large language models (LLMs). By applying the `@ai` decorator, developers can define functions where the core logic is delegated to an LLM, while maintaining type safety, input/output validation, and seamless integration with Python code. The library builds on DSPy for LLM orchestration, providing abstractions for prompts, adapters, modules (e.g., Chain-of-Thought, ReAct), tools, state management, and optimization.

### Vision and Developer Experience
The ultimate goal of FunctAI is to make AI integration as natural as writing standard Python functions. Developers should be able to:
- Write typed functions with docstrings that serve as prompts.
- Use a sentinel object (`_ai`) to inject AI-generated values effortlessly.
- Configure LLM backends, reasoning strategies, and tools with minimal boilerplate.
- Optimize functions in-place using datasets for few-shot learning or fine-tuning.
- Preview prompts, inspect histories, and manage conversational state transparently.

In this polished vision, we've resolved potential issues from the original implementation:
- **Simplified Configuration**: Global and contextual defaults are streamlined; overrides are intuitive and chainable.
- **Robust Signature Building**: Automatically infers inputs/outputs from type hints, docstrings, and `_ai` usages, with fallback to explicit declarations.
- **Error Handling and Safety**: Adds runtime checks for LLM failures, type mismatches, and invalid configurations.
- **Enhanced State Management**: Stateful functions maintain conversation history with configurable windows, serialization, and reset mechanisms.
- **Optimization Workflow**: `.opt()` supports multiple strategies (e.g., BootstrapFewShot, BayesianOptimization) and allows undoing steps without losing history.
- **Interoperability**: Seamless with popular LLMs (e.g., OpenAI, Anthropic) via DSPy, plus custom adapters.
- **Debugging Tools**: Built-in prompt formatting, signature inspection, and history logging for easy debugging.

FunctAI aims for a "zero-boilerplate" experience where AI feels like a native Python feature, while providing power-user controls.

### Key Principles
- **Type-Safe AI**: Leverages Python type hints for input/output schemas.
- **Modular**: Swap LLMs, adapters, and reasoning modules (e.g., Predict, CoT, ReAct) easily.
- **Optimizable**: Compile functions with datasets to improve performance.
- **Stateful by Default (Opt-In)**: Supports conversational AI with history tracking.
- **Extensible**: Add tools (e.g., web search, code execution) for agentic behaviors.

### Installation
```bash
pip install functai
```
Requires DSPy (`pip install dspy-ai`) and an LLM provider (e.g., `pip install openai` for GPT models).

## Core API

### The `@ai` Decorator
The `@ai` decorator turns a function into an AI program. The function's docstring becomes the system prompt, parameters are inputs, and the return type is the primary output. Use `_ai` to access AI-generated results.

#### Basic Usage
```python
from functai import ai, _ai

@ai
def summarize(text: str) -> str:
    """Summarize the given text in one sentence."""
    return _ai  # AI generates the summary

result = summarize("Long article about climate change...")
print(result)  # Expected: "Climate change poses significant risks to global ecosystems and requires immediate action."
```

- **Behavior**: The function calls an LLM (default: configured global, e.g., GPT-4) with the docstring as prompt and `text` as input. `_ai` resolves to the LLM's output, coerced to `str`.
- **Expected Output**: A string summary. If the LLM fails, raises `AIError` with retry suggestions.

#### With Post-Processing
```python
@ai
def classify_sentiment(text: str) -> str:
    """Classify the sentiment as positive, negative, or neutral."""
    raw = _ai
    return raw.lower()  # Post-process AI output

result = classify_sentiment("I love this product!")
print(result)  # Expected: "positive"
```

- **Behavior**: `_ai` fetches the AI response; you can manipulate it before returning.

#### Multiple Outputs
Declare additional outputs via `_ai[(name, desc, type)]` or assignments like `thought: str = _ai["Reason step-by-step."]`.
```python
@ai
def math_problem(question: str) -> int:
    """Solve the math problem and explain your reasoning."""
    thought: str = _ai["Reason step-by-step before answering."]
    answer = _ai  # Primary output
    return answer  # Only return the int; thought is internal

result = math_problem("What is 2 + 2?")
print(result)  # Expected: 4
```

- **Behavior**: Generates a signature with fields `thought: str` and `result: int` (primary). The LLM produces both; function returns only the primary.

## Configuration

### Global Configuration
Set defaults for all `@ai` functions.
```python
from functai import configure

configure(lm="openai/gpt-4o", temperature=0.7, adapter="json", module="cot")
```

- **Parameters**:
  - `lm`: LLM identifier (str) or instance (e.g., `dspy.OpenAI(model="gpt-4o")`).
  - `temperature`: Float for creativity (0.0-1.0).
  - `adapter`: "json" for structured output, "chat" for conversational, or custom DSPy adapter.
  - `module`: "predict" (basic), "cot" (Chain-of-Thought), "react" (ReAct for tools), or custom DSPy module.
  - `stateful`: Bool to enable history tracking (default: False).

### Contextual Overrides
Use `with defaults(...)` for temporary scopes.
```python
from functai import defaults

with defaults(lm="anthropic/claude-3-opus", temperature=0.0):
    result = some_ai_func(...)  # Uses Claude with low temperature
```

- **Behavior**: Overrides apply only within the `with` block; reverts afterward.

## Advanced Features

### Tools
Enable agentic behaviors with tools (functions callable by the AI).
```python
def search(query: str) -> str:
    """Mock web search."""
    return f"Results for {query}"

@ai(tools=[search], module="react")
def answer_question(question: str) -> str:
    """Answer the question using tools if needed."""
    return _ai

result = answer_question("What's the capital of France?")
# AI might call search("capital of France") internally.
print(result)  # Expected: "Paris"
```

- **Behavior**: If tools are provided and module is "react", the AI can invoke tools in a loop. Tools must be callable with type hints.

### State Management
For conversational functions.
```python
@ai(stateful=True)
def chat(message: str) -> str:
    """Respond to the user message."""
    return _ai

chat.state.enable(window=3)  # Keep last 3 turns
response1 = chat("Hello!")
response2 = chat("What's your name?")  # Remembers previous message
chat.state.clear()  # Reset history
```

- **Properties**:
  - `state.enabled`: Bool.
  - `state.window`: Int (history length).
  - `state.history`: List of dicts `{inputs: {...}, outputs: ...}`.
- **Behavior**: Appends recent turns to the prompt. Serializable via `json.dumps(chat.state.history)`.

### Optimization
Compile the function with a dataset for better performance.
```python
trainset = [
    dspy.Example(input={"text": "Example text"}, output="Summary").with_inputs("text")
]

summary_func.opt(trainset=trainset, strategy="bootstrap_fewshot", metric=some_metric)
optimized_result = summary_func("New text...")  # Uses optimized prompts/examples

summary_func.undo_opt()  # Revert to previous version
```

- **Parameters** (in `.opt()`):
  - `trainset`: List of DSPy Examples.
  - `strategy`: "bootstrap_fewshot" (default), "bayesian", etc.
  - Additional DSPy optimizer kwargs.
- **Behavior**: Mutates the function in-place; tracks history for undo. Raises if no trainset provided.

## Utilities

### Prompt Formatting
Preview the generated prompt.
```python
prompt = format_prompt(summarize, text="Sample text")
print(prompt["render"])
# Expected: Markdown-like string with system/user messages.
```

- **Returns**: Dict with `adapter`, `module`, `inputs`, `messages`, `render`, `signature`.

### Signature Computation
Inspect the inferred DSPy Signature.
```python
sig = compute_signature(summarize)
print(signature_text(summarize))
# Expected:
# Signature: SummarizeSig
# Doc: Summarize the given text in one sentence.
# Inputs:
# - text: str
# Outputs:
# - result: str (primary)
```

### History Inspection
View recent LLM interactions.
```python
print(inspect_history_text())
# Expected: Text dump of DSPy history (prompts, responses).
```

## Error Handling and Best Practices
- **Common Errors**: `AIError` for LLM failures (with retries), `TypeMismatchError` if output doesn't match hint.
- **Best Practices**: Use docstrings for clear prompts; type hints for schemas; test with `format_prompt` before running.
- **Performance**: Cache responses via DSPy config; use low temperature for deterministic outputs.

## Future Enhancements
- Async support for concurrent AI calls.
- Integration with more LLMs (e.g., local models via HuggingFace).
- Visual prompt editor and history browser.
- Auto-optimization with RLHF-style feedback loops.

This spec defines a polished, user-friendly FunctAI, resolving original implementation gaps for a seamless DX.



# FunctAI Specification: The Function-is-the-Prompt Paradigm

## 1\. Introduction and Vision

FunctAI reimagines the integration of Large Language Models (LLMs) into Python applications. It allows developers to treat LLMs as reliable, typed Python functions.

The core philosophy of FunctAI is: **The function definition *is* the prompt, and the function body *is* the program definition.**

FunctAI abstracts away the complexities of explicit prompt engineering, output parsing, and execution strategy management. By leveraging Python's native features—docstrings for instructions, type hints for structure, and variable assignments for program flow—developers can define sophisticated AI behaviors within a simple, ergonomic function definition.

Built on the DSPy framework, FunctAI enables advanced strategies like Chain-of-Thought, ReAct (tool usage), and automatic optimization, all accessible through a simple decorator-based API.

## 2\. Getting Started

### 2.1. Installation

```bash
pip install functai
# FunctAI relies on dspy as a peer dependency
pip install dspy-ai
```

### 2.2. Configuration

Before using FunctAI, configure a default Language Model (LM).

```python
import dspy
from functai import configure

# Initialize a DSPy LM provider (e.g., OpenAI)
# Ensure your OPENAI_API_KEY environment variable is set
gpt4o = dspy.OpenAI(model='gpt-4o')

# Configure FunctAI globally
# adapter="json" is recommended for robust structured output
configure(lm=gpt4o, temperature=0.0, adapter="json")
```

### 2.3. Your First AI Function

Define a standard Python function with type hints and a docstring, and decorate it with `@ai`.

```python
from functai import ai, _ai

@ai
def summarize(text: str, focus: str = "key points") -> str:
    """Summarize the text, concentrating on the specified focus area."""
    # The _ai sentinel represents the LLM output
    return _ai

# Call it like a normal Python function
long_text = "FunctAI bridges the gap between Python's expressive syntax and the dynamic capabilities of LLMs. It allows developers to focus on logic rather than boilerplate."
summary = summarize(long_text, focus="developer benefits")
print(summary)
```

**Behavior:** FunctAI intercepts the call, constructs a prompt using the docstring and inputs (`text`, `focus`), invokes the configured LM, and returns the result matching the return type (`str`).

## 3\. Core Concepts

### 3.1. The `@ai` Decorator

The `@ai` decorator transforms a Python function into an LLM-powered program (a `FunctAIFunc` object). It accepts optional arguments to override defaults.

```python
@ai(lm="claude-3.5-sonnet", temperature=0.7, module="cot")
def creative_story(topic: str) -> str:
    """Write a creative short story about the topic."""
    return _ai
```

### 3.2. The `_ai` Sentinel

The `_ai` object is a special sentinel used within an `@ai` function to represent values generated by the AI. It acts as a proxy, deferring the actual LM execution until the value is needed.

#### 3.2.1. Direct Return

Returning `_ai` (or using Ellipsis `...` or an empty body) indicates the LLM's output is the function's final result.

#### 3.2.2. Post-processing

You can assign `_ai` to a variable and apply standard Python operations. `_ai` behaves dynamically like the expected return type.

```python
@ai
def sentiment_score(text: str) -> float:
    """Returns a sentiment score between 0.0 (negative) and 1.0 (positive)."""
    # _ai behaves like a float here due to the return type hint
    score = _ai
    # Post-processing: ensure the score is within bounds
    return max(0.0, min(1.0, float(score)))
```

#### 3.2.3. Defining Intermediate Outputs

Assigning `_ai` to typed variables within the function body defines intermediate outputs for the LM, crucial for techniques like Chain-of-Thought (see Section 5.1).

```python
# Syntax: variable_name: Type = _ai["Description of the step"]
think: str = _ai["Step-by-step reasoning."]
```

## 4\. Structured Output

FunctAI excels at extracting structured data. By specifying complex types as the return type, the LM is guided (especially when using `adapter="json"`) to produce output conforming to that schema.

### 4.1. Dataclasses and Lists

```python
from dataclasses import dataclass
from typing import List
from functai import ai, _ai

@dataclass
class Ingredient:
    name: str
    quantity: float
    unit: str

@ai(adapter="json") # Explicitly ensuring JSON adapter
def extract_ingredients(text: str) -> List[Ingredient]:
    """Extracts ingredients from recipe text."""
    return _ai

text = "You need 5 apples, 1 cup sugar, and 1 tbsp cinnamon."
ingredients = extract_ingredients(text)

# Behavior: The output is a validated list of Ingredient instances.
print(ingredients[0].name)
# Expected Output: apples
```

### 4.2. Enums

Enums can be used to restrict outputs to a predefined set of values.

```python
from enum import Enum

class Sentiment(Enum):
    POSITIVE = "positive"
    NEGATIVE = "negative"
    NEUTRAL = "neutral"

@ai
def classify_sentiment(text: str) -> Sentiment:
    """Analyzes the text and classifies its sentiment."""
    return _ai

result = classify_sentiment(text="The new features are fantastic!")
# Expected Output: <Sentiment.POSITIVE: 'positive'>
```

## 5\. Advanced Execution Strategies

FunctAI allows defining complex execution strategies directly within the function body or via configuration.

### 5.1. Chain of Thought (CoT)

Eliciting reasoning (Chain of Thought) often improves the quality of the final answer.

#### 5.1.1. Explicit CoT (Inline Definition)

The most intuitive way to define CoT in FunctAI is by explicitly declaring reasoning steps within the function body using `_ai` assignments.

```python
@ai
def solve_math_problem(question: str) -> float:
    """Solves a math word problem and returns the numerical answer."""

    # Define the reasoning step.
    # The variable name ('reasoning') is the field name.
    # The type hint (str) defines the output type.
    # The subscript _ai[...] provides a description for the LLM.
    reasoning: str = _ai["Step-by-step thinking process to reach the solution."]

    # The final return value (float) is the main output
    return _ai
```

**Behavior:** FunctAI analyzes the function body, detects the intermediate assignment, and automatically configures the underlying module to generate the `reasoning` before the final result.

#### 5.1.2. Accessing Intermediate Steps

While the function call returns the final result, you can access the intermediate steps (like `reasoning`) by requesting the raw prediction object using the `_prediction=True` argument.

```python
question = "If a train travels 120 miles in 2 hours, what is its speed?"
prediction = solve_math_problem(question, _prediction=True)

print(prediction.reasoning)
# Example Output: 1. Identify distance (120 miles) and time (2 hours). 2. Use formula speed = distance/time...
print(prediction.result) # The main output (if returned as _ai directly)
# Example Output: 60.0
```

#### 5.1.3. Implicit CoT (Configuration)

Alternatively, you can enable CoT via configuration, which generates an internal, unnamed thought process.

```python
@ai(module="cot")
def analyze(data: str) -> str:
    """Analyze the data and provide a conclusion."""
    return _ai
```

### 5.2. Tool Usage (ReAct Agents)

FunctAI supports the ReAct pattern for integrating external tools. When the `tools` argument is provided, the module automatically upgrades to `dspy.ReAct`. Tools can be standard Python functions.

```python
# Define a tool
def search_web(query: str) -> str:
    """Searches the web for information."""
    # Placeholder implementation
    return f"Search results for '{query}'..."

# Define the AI function with access to the tool
@ai(tools=[search_web])
def research_topic(topic: str) -> str:
    """Research the given topic using available tools and summarize the findings."""
    return _ai

# Behavior: The function will iteratively use search_web until it has enough information.
summary = research_topic("History of the DSPy framework")
```

### 5.3. Multi-Output Functions

You can define and return multiple distinct outputs from a single function call by declaring them inline.

```python
from typing import Tuple

@ai
def critique_and_improve(text: str) -> Tuple[str, str]:
    """
    Analyze the text, provide constructive criticism, and suggest an improved version.
    """
    # Define explicit output fields
    critique: str = _ai["Constructive criticism focusing on clarity and tone."]
    improved_text: str = _ai["The improved version of the text."]

    # Return the materialized fields
    return critique, improved_text

critique, improved = critique_and_improve(text="U should fix this asap, it's broken.")
```

## 6\. Configuration and Flexibility

FunctAI uses a flexible, cascading configuration system. Settings are applied in the following order of precedence: Function-Level \> Contextual \> Global.

### 6.1. Global Configuration

Use `functai.configure()` for project-wide defaults.

```python
import functai

functai.configure(
    lm="gpt-3.5-turbo",
    temperature=0.5,
    adapter="json",       # Default adapter ("json", "chat", or dspy.Adapter instance)
    module="predict",     # Default module ("predict", "cot", "react")
    stateful=False
)
```

### 6.2. Contextual Defaults

Use the `functai.defaults()` context manager to temporarily override defaults.

```python
from functai import defaults

@ai
def analyze(data): return _ai

analyze("data1") # Uses global defaults

with defaults(temperature=0.9, module="cot"):
    analyze("data2") # Uses temperature=0.9 and CoT
```

### 6.3. Live Modification and Persona

`FunctAIFunc` objects have mutable properties, allowing configuration changes after definition. You can also set a `persona` to define the AI's role, which is prepended to the instructions.

```python
@ai
def translator(text: str) -> str:
    """Translate text to French."""
    return _ai

# Change the model and temperature on the fly
translator.lm = "claude-3-opus"
translator.temperature = 0.2

# Set a specific persona
translator.persona = "You are an expert literary translator specializing in French literature."
```

## 7\. Stateful Interactions (Memory)

By default, AI functions are stateless. To maintain context across calls (e.g., in a chatbot scenario), set `stateful=True`.

```python
@ai(stateful=True)
def chatbot(user_message: str) -> str:
    """A friendly chatbot that remembers the conversation history."""
    return _ai

response1 = chatbot("Hello, my name is Alex.")
response2 = chatbot("What is my name?")
# Behavior: response2 will know the name is Alex because the history is included in the context.
```

You can manage the state explicitly: `chatbot.state.clear()`.

## 8\. Optimization (In-place Compilation)

FunctAI integrates with DSPy's optimization capabilities (Teleprompters). Optimization (compilation) improves quality by generating few-shot examples or refining prompts based on a dataset. This happens *in place* using the `.opt()` method.

```python
from dspy import Example

# 1. Define the function
@ai
def classify_sentiment(text: str) -> str:
    """Classify sentiment as 'positive' or 'negative'."""
    return _ai

# 2. Define the training data
trainset = [
    Example(text="I loved this movie!", result="positive").with_inputs("text"),
    Example(text="It was terrible.", result="negative").with_inputs("text"),
]

# 3. Optimize the function in place
# strategy="launch" typically uses BootstrapFewShot
classify_sentiment.opt(trainset=trainset, strategy="launch")

# 4. The function is now optimized (e.g., includes few-shot examples)
result = classify_sentiment("It was quite enjoyable.")
```

You can revert optimization steps using `.undo_opt()`:

```python
classify_sentiment.undo_opt(steps=1)
```

## 9\. Inspection and Debugging

FunctAI provides utilities to understand how prompts are constructed and executed.

### 9.1. Previewing the Prompt

Use `functai.format_prompt` to preview the messages sent to the LM for specific inputs.

```python
from functai import format_prompt

preview = format_prompt(summarize, text="A long document...", focus="key points")
print(preview['render'])
```

**Example Output (Simplified):**

```
Adapter: ChatAdapter
Module: Predict

System:
Summarize the text, concentrating on the specified focus area.

User:
[Inputs: text, focus]
[Output structure definition]
```

### 9.2. Inspecting the Signature

Use `functai.signature_text` to inspect the DSPy Signature derived from the function definition.

```python
from functai import signature_text
print(signature_text(summarize))
```

**Example Output:**

```
Signature: SummarizeSig
Doc: Summarize the text, concentrating on the specified focus area.
Inputs:
- text: <class 'str'>
- focus: <class 'str'>
Outputs:
- result: <class 'str'> (primary)
```

### 9.3. Inspecting History

Use `functai.inspect_history_text()` to get a textual representation of the most recent LM calls (leveraging `dspy.inspect_history()`).

## 10\. API Reference

  * `@ai(_fn=None, **cfg)`: Decorator to create an AI function (`FunctAIFunc`). Config options: `lm`, `temperature`, `adapter`, `module`, `tools`, `stateful`.
  * `_ai`: Sentinel representing the LM output.
      * `_ai["description"]`: Used to define intermediate or explicit output fields with instructions.
  * `configure(**cfg)`: Set global defaults.
  * `defaults(**overrides)`: Context manager for temporary default overrides.
  * `settings`: Direct access to the global defaults object.
  * `FunctAIFunc`: The callable object returned by `@ai`.
      * `__call__(*args, _prediction=False, **kwargs)`: Executes the program. If `_prediction=True`, returns the full `dspy.Prediction` object.
      * `.opt(trainset, strategy="launch", **opts)`: Optimize the function in place.
      * `.undo_opt(steps=1)`: Revert optimization steps.
      * `.state`: Access state management (`.clear()`, `.history`).
      * Mutable properties: `lm`, `temperature`, `adapter`, `module`, `tools`, `persona`.
  * Utilities:
      * `format_prompt(fn_or_prog, /, **inputs)`: Preview the formatted prompt.
      * `compute_signature(fn_or_prog)`: Compute and return the `dspy.Signature`.
      * `signature_text(fn_or_prog)`: Return a human-readable summary of the signature.
      * `inspect_history_text()`: Return the LM call history as text.



</all specs drafts>